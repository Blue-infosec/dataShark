{
  "name": "dataShark",
  "tagline": "dataShark is a Security & Network Event Analytics Framework built on Apache Spark",
  "body": "Welcome to dataShark\r\n====================\r\n\r\nTable of Contents\r\n-----------------\r\n\r\n  * [dataShark](#datashark)\r\n  * [Getting Started](#getting-started)\r\n      * [The Installer Script](#the-installer-script)\r\n      * [The Docker Image](#the-docker-image)\r\n      * [The Manual Method](#the-manual-method)\r\n        * [Installing Dependencies](#installing-dependencies)\r\n        * [Running dataShark in Standalone Mode](#running-datashark-in-standalone-mode)\r\n        * [Running dataShark in Production Mode](#running-datashark-in-production-mode)\r\n  * [Structure of dataShark](#structure-of-datashark)\r\n    * [The Directory Structure](#the-directory-structure)\r\n      * [datashark.py](#datasharkpy)\r\n      * [datashark.conf](#datasharkconf)\r\n      * [start.sh](#startsh)\r\n      * [datashark_standalone.py](#datashark_standalonepy)\r\n      * [datashark-env.sh](#datashark-envsh)\r\n      * [install.sh](#installsh)\r\n      * [standalone.sh](#standalonesh)\r\n      * [conf/](#conf)\r\n      * [conf/wordcount/wordcount[.py|.conf|.txt]](#confwordcountwordcountpyconftxt)\r\n      * [lib/](#lib)\r\n      * [plugins/output/](#pluginsoutput)\r\n  * [Writing your own use cases using dataShark](#writing-your-own-use-cases-using-datashark)\r\n    * [The .conf File](#the-conf-file)\r\n      * [Required Keys](#required-keys)\r\n      * [Optional Keys](#optional-keys)\r\n    * [The .py file](#the-py-file)\r\n  * [Output Plugins](#output-plugins)\r\n      * [1. Elasticsearch Output Plugin](#1-elasticsearch-output-plugin)\r\n      * [2. Syslog Output Plugin](#2-syslog-output-plugin)\r\n      * [3. CSV Output Plugin](#3-csv-output-plugin)\r\n  * [Input Plugins](#input-plugins)\r\n      * [1. Kafka Input Plugin](#1-kafka-input-plugin)\r\n      * [2. File Input Plugin](#2-file-input-plugin)\r\n  * [Uninstall dataShark](#uninstall-datashark)\r\n  * [Changelog](#changelog)\r\n  * [Contacts](#contacts)\r\n  * [License](#license)\r\n  * [Authors](#authors)\r\n\r\ndataShark\r\n=========\r\n\r\ndataShark is a Security & Network Event Analytics Framework built on Apache Spark that enables security researchers, big data analysts and operations teams to carry out the below tasks with minimal effort:\r\n\r\n1. Data ingest from various sources such as file system, Syslog, Kafka.\r\n2. Write custom map / reduce and ML algorithms that operate on ingested data using abstracted Apache Spark functionality.\r\n3. The output of the above operations can be sent to destinations such as syslog, elasticsearch and can also be persisted in the file system or HDFS.\r\n\r\ndataShark has the following two operation modes:\r\n\r\n1. **Standalone executable**: allows one-shot analysis of static data (input can be file system or HDFS and output can be any of the available output plugins - Syslog, Elasticsearch, file system, HDFS).\r\n2. **Production**: full-fledged production deployment with all components (refer next section) that can ingest data from the mentioned sources.\r\n\r\nWe recommend the following components while running dataShark in production:\r\n\r\n1. *Event Acquisition*: this layer leverages Fluentd for processing events via syslog, parsing them as per user-defined grok patterns and forwarding the same to Kafka for queuing.\r\n2. *Event Queuing*: this layer uses Kafka to queue events for Spark to process. Often useful when the input EPS is high and the Spark layer has a delay in processing.\r\n3. *Core Data Engine*: this is the core dataShark framework built on Spark. A user can deploy custom map / reduce or ML use cases and submit the output via any available output plugin.\r\n4. *Persistence Layer*: elasticsearch and HDFS are leveraged to persist output and lookup data as required.\r\n5. *Alerting Engine*: this uses ElastAlert to monitor output data and configure custom rules / thresholds to generate alerts.\r\n6. *Output Plugins*: these are used by the Core Data Engine to persist or send its output to various destinations such as CSV, Syslog or Elasticsearch.\r\n\r\n> *Note*: 1, 2, 4 and 5 are purely basis our experience with the production setup. One is free to use any available alternative.\r\n\r\n[![](https://makemytrip.github.io/images/dataShark_arch.jpg)]()\r\n\r\n**Input Data Sources:**\r\n\r\n1. *File system*: static files to carry out one-time analysis or to train data models for ML.\r\n2. *Kafka*: stream data that is ingested using Spark stream processing capabilities.\r\n\r\n**Output plugins:**\r\n\r\n1. *CSV (File system or HDFS)*: persist output data in CSV file format on the file system or HDFS.\r\n2. *Syslog*: send output data to a remote syslog server.\r\n3. *Elasticsearch*: persist output data in elasticsearch.\r\n\r\n**Sample use cases / implementations:**\r\n\r\n1. Detecting bots or anomalous server behavior using K-means classification and anomaly detection techniques.\r\n2. Generating a summary of websites accessed using map / reduce on web proxy logs.\r\n3. Generating a summary of web and network traffic using map / reduce on Apache HTTPD and firewall logs.\r\n\r\nGetting Started\r\n=============\r\n\r\nThis section describes how to get started with dataShark. Before you can actually run dataShark, there are some prerequisites that need to be installed. \r\n\r\ndataShark can run in 2 modes:\r\n\r\n1. Standalone Mode\r\n2. Production Mode\r\n\r\nFollowing are the requirements for Standlone Mode Setup:\r\n\r\n1. pySpark\r\n2. Python Dependencies\r\n\r\nFollowing are the requirements for Production Mode Setup:\r\n\r\n1. pySpark\r\n2. Python Dependencies\r\n3. Kafka\r\n4. Hadoop\r\n5. Fluent\r\n\r\n## The Installer Script\r\n\r\nSetting up dataShark to run in Standalone Mode is as easy as running the installer script. We now have the install.sh script that installs all required dependencies for dataShark Standalone Mode (to run the included wordcount use case). Following components get installed using the install.sh script:\r\n\r\n1. Spark 1.6.2\r\n2. Java 8u112\r\n3. Scala 2.11.8\r\n4. System Modules: git, gcc, Python Development Files, Python Setuptools\r\n5. Python Dependecies: py4j, configobj, argparse, numpy\r\n\r\nTo run the install script, just execute the following command inside the cloned dataShark directory:\r\n\r\n```\r\n./install.sh\r\n```\r\n\r\nThe install script, by default, installs all components to the directory /etc/datashark, but this can be overridden by providing the prefix flag like so: `./install.sh --prefix=/opt` will install to /opt/datashark.\r\n\r\n## The Docker Image\r\n\r\nWe wanted dataShark to be available as a ready-to-go package. Therefore, we also have a Docker Image with all dependencies and dataShark pre-installed on a base CentOS 6.8 to run the wordcount use case. The following commands can be used to pull dataShark Image from Docker Hub and drop into the image's shell:\r\n\r\n```\r\ndocker pull makemytrip/datashark\r\ndocker run -i -t makemytrip/datashark /bin/bash\r\n```\r\n\r\nThat's it! You're now inside the CentOS shell. dataShark is installed in /etc/datashark.\r\n\r\n## The Manual Method\r\n\r\n### Installing Dependencies\r\n\r\n1. **pySpark** is the brains of dataShark. We recommend installing Spark with Oracle's Java >= 1.8.0. Use the following link to setup up pySpark: [https://github.com/KristianHolsheimer/pyspark-setup-guide](https://github.com/KristianHolsheimer/pyspark-setup-guide). This document provides the easiest method of setting up Spark with pySpark.\r\n   - *[Optional]* Setting up Spark Cluster: Although it is highly recommended for better performance to setup Spark Cluster, this step is completely optional basis infrastructure availability. Following link provides steps to setup a basic Spark Cluster: [http://blog.insightdatalabs.com/spark-cluster-step-by-step/](http://blog.insightdatalabs.com/spark-cluster-step-by-step/).\r\n   - *Note*: It is required to set the SPARK_HOME environment variable. Make sure you set it as mentioned in the install guide mentioned above.\r\n2. **Python Dependencies** are needed to be installed to run dataShark. To install the required dependencies, run the following command: `pip install configobj argparse`\r\n3. **Kafka** provides the high throughput message queue functionality which gets loaded to dataShark as a continuous stream for analysis. Kafka can be setup either with multiple brokers across multiple nodes or on a single node with multiple brokers, the basic setup steps are fairly the same for both methods. Following link can be used to setup multiple brokers on a single node: [http://www.michael-noll.com/blog/2013/03/13/running-a-multi-broker-apache-kafka-cluster-on-a-single-node/](http://www.michael-noll.com/blog/2013/03/13/running-a-multi-broker-apache-kafka-cluster-on-a-single-node/).\r\n4. **Hadoop** is the resilient storage for spark that's used for checkpointing and also as a driver for starting up dataShark with all dependencies. Following link provides steps to setup a multi-node Hadoop Cluster: [http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/](http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/).\r\n5. **Fluentd** is the data aggregator that collects logs from multiple sources and sends them to Kafka for consumption. Setting up fluentd is pretty easy. Fluentd's website provides steps for installation for all platforms. The Installation document can be found here: [http://docs.fluentd.org/v0.12/categories/installation](http://docs.fluentd.org/v0.12/categories/installation).\r\n\r\n### Running dataShark in Standalone Mode\r\n\r\ndataShark has a standalone mode for testing out use cases before plugging them in. The basic requirement to get dataShark running in standalone mode is having Apache Spark installed on the system hosting dataShark. \r\n\r\nOnce we have all these prerequisites setup, to run dataShark in Standalone Mode you first need to clone this git repository:\r\n\r\n```\r\ngit clone https://github.com/makemytrip/dataShark.git\r\n```\r\n\r\nAfter cloning, executing a use case in standalone mode is as easy as running the following command:\r\n\r\n```\r\n./standalone.sh conf/wordcount/wordcount.conf\r\n```\r\n\r\nFollowing is the sample output for a sample Standalone Mode run:\r\n\r\n```\r\n\r\n               __      __        _____ __               __\r\n          ____/ /___ _/ /_____ _/ ___// /_  ____ ______/ /__\r\n         / __  / __ `/ __/ __ `/\\__ \\/ __ \\/ __ `/ ___/ //_/\r\n        / /_/ / /_/ / /_/ /_/ /___/ / / / / /_/ / /  / ,<\r\n        \\__,_/\\__,_/\\__/\\__,_//____/_/ /_/\\__,_/_/  /_/|_|\r\n\r\n                          STANDALONE MODE\r\n\r\n                               v1.1\r\n\r\n\r\nLoaded Confs: ['conf/wordcount/wordcount.conf']\r\n2016-10-14 13:01:39 - (Word Count) - Written 1384 Documents to CSV File /tmp/usecase1.csv\r\n```\r\n\r\nThe Standalone Mode now also provides a debugging feature, where the output is not passed to the output plugin, rather printed out on the console using collect() for batch mode use cases and pprint() for streaming use cases. This can help in quickly debugging a use case than having to use the output plugin. The debug mode can be turned on using the `-d` flag and passing the conf file as given below:\r\n\r\n```\r\n./standalone.sh -d conf/wordcount/wordcount.conf\r\n```\r\n\r\nFollowing is the sample output for a sample Standalone Mode run in debugging mode:\r\n\r\n```\r\n\r\n               __      __        _____ __               __\r\n          ____/ /___ _/ /_____ _/ ___// /_  ____ ______/ /__\r\n         / __  / __ `/ __/ __ `/\\__ \\/ __ \\/ __ `/ ___/ //_/\r\n        / /_/ / /_/ / /_/ /_/ /___/ / / / / /_/ / /  / ,<\r\n        \\__,_/\\__,_/\\__/\\__,_//____/_/ /_/\\__,_/_/  /_/|_|\r\n\r\n                          STANDALONE MODE\r\n\r\n                               v1.1\r\n\r\n\r\n[*] Debug Mode is ON\r\nLoaded Confs: ['conf/wordcount/wordcount.conf']\r\n[(u'all', 20), (u'moreover,', 1), (u'\"recipients\"', 1), (u'(if', 3), (u'party.', 1), (u'procuring', 1), (u'provided,', 1), (u'methods,', 1), (u'versions.', 2), (u'presents', 1), (u'violation', 5), (u'charge', 5), (u'permanently', 3), (u'those', 14)\r\n...\r\n```\r\n\r\n### Running dataShark in Production Mode\r\n\r\nOnce we have all these prerequisites setup, to run dataShark in Production Mode you first need to clone this git repository:\r\n\r\n```\r\ngit clone https://github.com/makemytrip/dataShark.git\r\n```\r\n\r\nThen we need to change the `datashark.conf` to specify the Zookeeper Host, Kafka Topic Name and HDFS Host Information. An example of sample configuration is given below:\r\n\r\n```\r\nzookeeper.host = 127.0.0.1\r\nzookeeper.port = 2181\r\n\r\nkafka.consumer.name = consumer-datashark\r\nkafka.queue.name = logs_queue\r\nkafka.partitions = 1\r\n\r\nhdfs.host = 127.0.0.1\r\nhdfs.port = 9000\r\n```\r\n\r\nThen we need to specify the Spark Master in datashark-env.sh:\r\n\r\n```\r\nSPARK_INSTANCE=spark://127.0.0.1:7077\r\n```\r\n\r\nThis is all that is needed to install and configure datashark. To start the spark engine and use cases, we can simply run the command:\r\n\r\n```\r\n./start.sh\r\n```\r\n\r\nOn the first run, we only have the Word Count Use Case enabled that will be run.\r\nFollowing is the sample output of the first run:\r\n\r\n```\r\n\r\n               __      __        _____ __               __\r\n          ____/ /___ _/ /_____ _/ ___// /_  ____ ______/ /__\r\n         / __  / __ `/ __/ __ `/\\__ \\/ __ \\/ __ `/ ___/ //_/\r\n        / /_/ / /_/ / /_/ /_/ /___/ / / / / /_/ / /  / ,<\r\n        \\__,_/\\__,_/\\__/\\__,_//____/_/ /_/\\__,_/_/  /_/|_|\r\n\r\n                          PRODUCTION MODE\r\n\r\n                               v1.1\r\n\r\n\r\n[*] 2016-10-12 14:59:06 Running Spark on Cluster\r\n[*] 2016-10-12 14:59:06 Preparing FileSystem\r\n[*] 2016-10-12 14:59:12 Starting Spark\r\nLoaded Confs: ['/opt/dataShark/conf/wordcount/wordcount.conf']\r\n2016-10-12 14:59:21 - (Word Count) - Written 1384 Documents to CSV File /tmp/usecase1.csv\r\n * Skip Stream Processing\r\n[*] 2016-10-12 14:59:23 Cleaning Up\r\n```\r\n\r\nStructure of dataShark\r\n======================\r\n\r\nThis section describes the structure of directories and files.\r\n\r\nThe Directory Structure\r\n-----------------------\r\n\r\nFollowing is the basic directory structure of dataShark after deployment.\r\n\r\n```\r\ndataShark/\r\n├── conf\r\n│   ├── __init__.py\r\n│   └── wordcount\r\n│       ├── __init__.py\r\n│       ├── wordcount.conf\r\n│       ├── wordcount.py\r\n│       └── wordcount.txt\r\n├── datashark.conf\r\n├── datashark.py\r\n├── datashark_standalone.py\r\n├── datashark-env.sh\r\n├── install.sh\r\n├── lib\r\n│   ├── elasticsearch-hadoop-2.2.0.jar\r\n│   └── spark-streaming-kafka-assembly-1.6.1.jar\r\n├── plugins\r\n│   ├── __init__.py\r\n│   └── output\r\n│       ├── __init__.py\r\n│       ├── out_csv.py\r\n│       ├── out_elasticsearch.py\r\n│       └── out_syslog.py\r\n├── start.sh\r\n└── standalone.sh\r\n```\r\n\r\nFiles and directories have been explained below:\r\n\r\n### datashark.py\r\nThe heart of dataShark, this is where the magic happens. When using dataShark, you will *never* have to change the contents of this file.\r\n\r\n### datashark.conf\r\nThe main configuration file which specifies the Kafka Queue to be consumed for Streaming Use Cases.\r\n\r\n### start.sh\r\nThe shell file that is used to start up spark with all its dependencies and use cases.\r\n\r\n### datashark_standalone.py\r\nThis file is used for testing individual use cases before integrating them as plugins to dataShark. \r\n\r\n### datashark-env.sh\r\nThis file gets loaded before execution of any Use Cases. Here you may place any required environment variables to be used by dataShark. One important variable to be set in this file is the SPARK_INSTANCE variable, that sets the spark master Host and Port.\r\n\r\n### install.sh\r\nThe Installer Script for installing requirements required to run dataShark in Standalone Mode to execute the included wordcount use case.\r\n\r\n### standalone.sh\r\nThis script is the helper for datashark_standalone.py. This loads all environment variables for executing a Use Case. You can write an use case as usual and execute it using the command, `./standalone.sh conf/use_case_dir/sample_use_case.conf` to just execute that use case. This file ignores the enabled flag in the conf, so it is advised to keep the flag set to false while doing a dry run to avoid accidental execution of the use case.\r\n\r\n### conf/\r\nThis directory is where all use cases are placed. Refer to [Writing your own use cases using dataShark](#writing-your-own-use-cases-using-datashark) on how to write your own plugin use cases.\r\n\r\n### conf/wordcount/wordcount[.py|.conf|.txt]\r\nWordcount is a sample use case provided with dataShark for trying out batch processing. The wordcount.txt file is a standard GNUv3 License file.\r\n\r\n### lib/\r\nAny external jar dependencies that need to be included with spark for your use cases. Out of the box, we provide you 2 jars included with datashark, Spark Streaming Kafka Library and Elasticsearch Library.\r\n\r\n### plugins/output/\r\nAll output plugins are placed in this directory. The naming convention for files in this directory is `out_<plugin name>.py`, example, out_csv.py. Three output plugins are provided out of the box:\r\n1. Elasticsearch\r\n2. Syslog\r\n3. CSV\r\n\r\nWriting your own use cases using dataShark\r\n===================\r\n\r\nThis document describes how to create your own use cases using dataShark. All custom code resides in the `conf` folder of dataShark.\r\n\r\nFirst create a folder with the use case name (all lower case characters and underscore only) in the conf folder. To write a new use case, at the bare minimum, we need 2 files in its own folder:\r\n\r\n1. The **.conf file**\r\n2. A code **.py file**\r\n\r\nThe .conf file needs to define a few necessary flags specific to the use case and the .py file needs to implement the `load` function.\r\n\r\nThe .conf File\r\n--------------------\r\n\r\nThis is what a standard .conf file looks like:\r\n\r\n```\r\nname = Use Case 1\r\ntype = streaming\r\ncode = code.py\r\nenabled = true\r\ntraining = training.log\r\noutput = elasticsearch\r\ninput = kafka\r\n[in_kafka]\r\n\thost = 127.0.0.1\r\n        port = 2181\r\n        topic = logs_queue\r\n        partitions = 1\r\n[log_filter]\r\n        [[include]]\r\n                some_key = ^regex pattern$\r\n        [[exclude]]\r\n                some_other_key = ^regex pattern$\r\n[out_elasticsearch]\r\n        host = 10.0.0.1\r\n        port = 9200\r\n        index_name = logstash-index\r\n        doc_type = docs\r\n        pkey = sourceip\r\n        score = anomaly_score\r\n        title = Name for ES Document\r\n        debug = false\r\n```\r\n\r\n### Required Keys\r\n\r\n```\r\nname : Name for the Use Case.\r\ntype : This can be either of **batch** or **streaming**. When using *batch* mode, the use case is run just once on the provided data file. In *streaming* mode a Kafka Stream is passed to the code file to analyze.\r\nenabled : Set this to either **true** or **false** to simply enable or disable this use case.\r\ncode : The .py file corresponding to this use case.\r\ntraining : The log file to supply as training data to train your model. This is required only when `type = streaming`. (In batch mode, this key can be skipped)\r\nfile : The data file to use for *batch* processing. This is required only when `type = batch`. (In streaming mode, this key can be skipped)\r\noutput : The output plugin to use. Types of output plugins are listed below.\r\n[type_of_plugin] : The settings for the output plugin being used.\r\n```\r\n\r\n### Optional Keys\r\n\r\n```\r\ninput : This key is used to override the Global Kafka Stream. This can be either `kafka` or `file`.\r\n[in_file] or [in_kafka] : This is used to specify the input conf to override the Global Kafka Stream.\r\n[log_filter] : This is used to filter out the Kafka stream passed to your use case. It has the following two optional sub-sections:\r\n    - [[include]] : In this sub-section each key value pair is used to filter the incoming log stream to include in the use case. The *key* is the name of the key in the JSON Document in the Kafka Stream. The *value* has to be a regex pattern that matches the content of that key.\r\n    - [[exclude]] : In this sub-section each key value pair is used to filter the incoming log stream to exclude from the use case. The *key* is the name of the key in the JSON Document in the Kafka Stream. The *value* has to be a regex pattern that matches the content of that key.\r\n```\r\n\r\nThe .py file\r\n----------------\r\nThe .py file is the brains of the system. This is where all the map-reduce. model training happens. The user needs to implement a method named `load` in this .py file. dataShark provides two flavors of the load function to implement, one for streaming and one for batch processing. Following is the basic definition of the load function of each type:\r\n\r\n*For batch processing:*\r\n\r\n```\r\ndef load(batchData)\r\n```\r\nThe data file provided as input in the .conf file is loaded and passed to the function in the variable batchData. batchData is of type `PythonRDD`.\r\n\r\n*For stream processing:*\r\n\r\n```\r\ndef load(streamingData, trainingData, context)\r\n```\r\n\r\n`streamingData` is the Kafka Stream being sent to the function load. This is of type *DStream*.\r\n`trainingData` is the Training File loaded from the *training* key mention in the .conf file. It is of the type *PythonRDD*.\r\n`context` is the spark context loaded in the driver. It may be used for using the accumulator, etc.\r\n\r\nThe function `load` expects a processed *DStream* to be returned from it. Each RDD in the DStream should be in the following format (this format is necessary for usability in output plugins):\r\n`('primary_key', anomaly_score, {\"some_metadata\": \"dictionary here\"})`\r\n\r\n*primary_key* is a string. It is the tagging metric by which the data was aggregated for map-reduce and finally scored.\r\n*anomaly_score* is of type float. It is the value used to define the deviation from normal behavior.\r\n*metadata* is of the type dictionary. This is the extra data that needs to be inserted into the Elasticsearch document or added to the CSV as extra Columns.\r\n\r\nOutput Plugins\r\n=============\r\ndataShark provides the following 3 output plugins out-of-the-box for processed data persistence or transmission:\r\n\r\n1. Elasticsearch\r\n2. Syslog\r\n3. CSV\r\n\r\nEach of these plugins requires its own basic set of settings, described below.\r\n\r\n### 1. Elasticsearch Output Plugin\r\n\r\nThe Elasticsearch output plugin allows you to easily push JSON documents to your Elasticsearch Node. This allows users to build visualizations using Kibana over processed data.\r\n\r\nFollowing is the basic template for configuring Elasticsearch output plugin:\r\n\r\n```\r\noutput = elasticsearch\r\n[out_elasticsearch]\r\n        host = 127.0.0.1\r\n        port = 9200\r\n        index_name = usecase\r\n        doc_type = spark-driver\r\n        pkey = source_ip\r\n        score = anomaly_score\r\n        title = Use Case\r\n        debug = false\r\n```\r\n\r\nAll settings in the config are optional. Their default values are displayed in the config above.\r\n\r\n\t`host` : Host IP or Hostname or the ES server.\r\n\t`port` : Port Number of ES Server.\r\n\t`index_name` : Name of the index to push documents to for this use case.\r\n\t`doc_type` : Document Type Name for this use case.\r\n\t`pkey` : Primary Key Field name to show in ES Document.\r\n\t`score` : Anomaly Score Key Field Name to show in ES Document.\r\n\t`title` : The value of the title field in the ES Document.\r\n\t`debug` : Set this to true to display each JSON record being push to ES on the console.\r\n\r\n### 2. Syslog Output Plugin\r\n\r\nThe Syslog Output plugin outputs JSON documents to the specified Syslog Server IP and Port. Following is the sample configuration with default settings for the plugin (all settings are optional):\r\n\r\n```\r\noutput = syslog\r\n[out_syslog]\r\n        host = 127.0.0.1\r\n        port = 514\r\n        pkey = source_ip\r\n        score = anomaly_score\r\n        title = Use Case Title\r\n        debug = false\r\n```\r\n\r\nThe settings are similar to that of elasticsearch.\r\n\r\n### 3. CSV Output Plugin\r\n\r\nThe CSV Output Plugins writes and appends output from Spark Use Case to a specified CSV File. Following is the sample configuration with default settings of the plugin (all settings are optional):\r\n\r\n```\r\noutput = csv\r\n[out_csv]\r\n        path = UseCase.csv\r\n        separator = ,\r\n        quote_char = '\"'\r\n        title = Use Case\r\n        debug = false\r\n```\r\n\r\nInput Plugins\r\n=============\r\n\r\ndataShark allows different inputs to be used per use case. These can be used to override the Global Kafka Stream per use case. Currently, only two plugins are provided:\r\n\r\n1. Kafka\r\n2. File\r\n\r\nEach of these plugins requires its own basic set of settings, described below.\r\n\r\n### 1. Kafka Input Plugin\r\n\r\nThis plugin can be used to specify a different Kafka Queue or a totally different Kafka Host to pickup the input stream from. The plugin has some mandatory configuration. A sample configuration is given below and can be modified as per need:\r\n\r\n```\r\ninput = kafka\r\n[in_kafka]\r\n        host = 127.0.0.1\r\n        port = 2181\r\n        topic = logs_queue\r\n        partitions = 1\r\n```\r\n\r\nAll keys are mandatory and need to be defined.\r\n\r\n### 2. File Input Plugin\r\n\r\nThis plugin can be used to pickup the input stream from a local folder (as suggested by Spark). The folder is monitored for any new files and loads the data from them. The plugin has some mandatory configuration. A sample configuration is given below and can be modified as per need:\r\n\r\n```\r\ninput = file\r\n[in_file]\r\n        folder_path = /tmp/mydata\r\n```\r\n\r\nAll keys are mandatory and need to be defined.\r\n\r\nUninstall dataShark\r\n===================\r\n\r\ndataShark gets installed locally to /etc/datashark (default). If you had specified the prefix during install, the location might be different. An environment variable DATASHARK_HOME is set indicating the directory where dataShark resides. To uninstall dataShark, just remove the datashark directory by issuing the following command:\r\n\r\n```\r\nrm -fr $DATASHARK_HOME\r\n```\r\n\r\n> *Note*: Please take a backup of the conf folder before executing the above command to backup any custom use cases you may have placed in dataShark.  \r\n\r\nSome environment variables were also set in the bashrc file. You may remove the lines from `/etc/bashrc (for CentOS)` or `/etc/bash.bashrc (for Ubuntu)`. A typical installation adds the following lines to the bashrc file:\r\n\r\n```\r\n## ENVIRONMENT VARIABLES SET BY DATASHARK\r\nexport DATASHARK_HOME=/etc/datashark\r\nexport JAVA_HOME=/etc/datashark/java\r\nexport JRE_HOME=/etc/datashark/java/jre\r\nexport PATH=$PATH:/etc/datashark/java/bin:/etc/datashark/java/jre/bin\r\nexport PATH=$PATH:/etc/datashark/scala/bin\r\nexport PATH=$PATH:/etc/datashark/spark/bin\r\nexport SPARK_HOME=/etc/datashark/spark\r\nexport PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH\r\n## END OF VARIABLES BY DATASHARK\r\n```\r\n\r\nChangelog\r\n=========\r\n\r\n### v1.1\r\n\r\n * Adding a debug feature for standalone mode to quickly display results on console.\r\n * Adding Input plugins Kafka and File, which can be used to override the Global Kafka Stream per use case.\r\n * Changed the naming convention used in Output Plugin conf.\r\n * Fixing a bug where an empty output conf would cause dataShark to crash.\r\n\r\n### v1.0\r\n\r\n * Initial Release of dataShark\r\n\r\nContacts\r\n========\r\n\r\nDiscussion / Forum: [https://groups.google.com/forum/#!forum/datashark](https://groups.google.com/forum/#!forum/datashark)\r\nFor anything else: datashark@makemytrip.com\r\n\r\nLicense\r\n=======\r\n\r\nThis program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\r\n\r\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details. [http://www.gnu.org/licenses/](http://www.gnu.org/licenses/).\r\n\r\n[![](https://www.gnu.org/graphics/gplv3-127x51.png)](https://www.gnu.org/licenses/gpl.txt)\r\n\r\nAuthors\r\n=======\r\n\r\n- [Kunal Aggarwal](https://twitter.com/KunalAggarwal92)\r\n- [Dhruv Kalaan](https://twitter.com/dkalaan)\r\n- [Vikram Mehta](https://twitter.com/vikramm811)",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}